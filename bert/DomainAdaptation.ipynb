{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_labels\": 2,\n",
    "    \"hidden_dropout_prob\": 0.15,\n",
    "    \"hidden_size\": 768,\n",
    "    \"max_length\": 512,\n",
    "}\n",
    "\n",
    "training_parameters = {\n",
    "    \"batch_size\": 2,\n",
    "    \"epochs\": 1,\n",
    "    \"output_folder\": \"./models/\",\n",
    "    \"output_file\": \"model.bin\",\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"print_after_steps\": 5,\n",
    "    \"save_steps\": 5000,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review = self.df.iloc[index][\"text\"]\n",
    "        sentiment = self.df.iloc[index][\"sentiment\"]\n",
    "        sentiment_dict = {\n",
    "            \"pos\": 0,\n",
    "            \"neg\": 1,\n",
    "        }\n",
    "        label = sentiment_dict[sentiment]\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "                review,\n",
    "                add_special_tokens=True,\n",
    "                max_length= config[\"max_length\"],\n",
    "                pad_to_max_length=True,\n",
    "                return_overflowing_tokens=True,\n",
    "            )\n",
    "        if \"num_truncated_tokens\" in encoded_input and encoded_input[\"num_truncated_tokens\"] > 0:\n",
    "            # print(\"Attention! you are cropping tokens\")\n",
    "            pass\n",
    "\n",
    "        input_ids = encoded_input[\"input_ids\"]\n",
    "        attention_mask = encoded_input[\"attention_mask\"] if \"attention_mask\" in encoded_input else None\n",
    "\n",
    "        token_type_ids = encoded_input[\"token_type_ids\"] if \"token_type_ids\" in encoded_input else None\n",
    "\n",
    "\n",
    "\n",
    "        data_input = {\n",
    "            \"input_ids\": torch.tensor(input_ids),\n",
    "            \"attention_mask\": torch.tensor(attention_mask),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids),\n",
    "            \"label\": torch.tensor(label),\n",
    "        }\n",
    "\n",
    "        return data_input[\"input_ids\"], data_input[\"attention_mask\"], data_input[\"token_type_ids\"], data_input[\"label\"]\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "imdb_df = pd.read_csv(\"./data/imdb_train.tsv\", sep='\\t')\n",
    "source_dataset = ReviewDataset(imdb_df)\n",
    "source_dataloader = DataLoader(dataset = source_dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n",
    "\n",
    "amazon_df = pd.read_csv(\"./data/amazon_train.tsv\", sep= \"\\t\")\n",
    "target_dataset = ReviewDataset(amazon_df)\n",
    "target_dataloader = DataLoader(dataset = target_dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Reversal Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "class GradientReversalFn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        \n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DomainAdaptationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainAdaptationModel, self).__init__()\n",
    "        \n",
    "        num_labels = config[\"num_labels\"]\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "        self.sentiment_classifier = nn.Sequential(\n",
    "            nn.Linear(config[\"hidden_size\"], num_labels),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(config[\"hidden_size\"], 2),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "          self,\n",
    "          input_ids=None,\n",
    "          attention_mask=None,\n",
    "          token_type_ids=None,\n",
    "          labels=None,\n",
    "          grl_lambda = 1.0, \n",
    "          ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "            )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "\n",
    "        reversed_pooled_output = GradientReversalFn.apply(pooled_output, grl_lambda)\n",
    "\n",
    "        sentiment_pred = self.sentiment_classifier(pooled_output)\n",
    "        domain_pred = self.domain_classifier(reversed_pooled_output)\n",
    "\n",
    "        return sentiment_pred.to(device), domain_pred.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(logits, labels):\n",
    "    \n",
    "    predicted_labels_dict = {\n",
    "      0: 0,\n",
    "      1: 0,\n",
    "    }\n",
    "    \n",
    "    predicted_label = logits.max(dim = 1)[1]\n",
    "    \n",
    "    for pred in predicted_label:\n",
    "        predicted_labels_dict[pred.item()] += 1\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "    \n",
    "    return acc, predicted_labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset = \"imdb\", percentage = 5):\n",
    "    with torch.no_grad():\n",
    "        predicted_labels_dict = {                                                   \n",
    "          0: 0,                                                                     \n",
    "          1: 0,                                                                     \n",
    "        }\n",
    "        \n",
    "        dev_df = pd.read_csv(\"./data/\" + dataset + \"_dev.tsv\", sep = \"\\t\")\n",
    "        data_size = dev_df.shape[0]\n",
    "        selected_for_evaluation = int(data_size*percentage/100)\n",
    "        dev_df = dev_df.head(selected_for_evaluation)\n",
    "        dataset = ReviewDataset(dev_df)\n",
    "\n",
    "        dataloader = DataLoader(dataset = dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n",
    "\n",
    "        mean_accuracy = 0.0\n",
    "        total_batches = len(dataloader)\n",
    "        \n",
    "        for input_ids, attention_mask, token_type_ids, labels in dataloader:\n",
    "            inputs = {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\" : token_type_ids,\n",
    "                \"labels\": labels,\n",
    "            }\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(device)\n",
    "\n",
    "\n",
    "            sentiment_pred, _ = model(**inputs)\n",
    "            accuracy, predicted_labels = compute_accuracy(sentiment_pred, inputs[\"labels\"])\n",
    "            mean_accuracy += accuracy\n",
    "            predicted_labels_dict[0] += predicted_labels[0]\n",
    "            predicted_labels_dict[1] += predicted_labels[1]  \n",
    "        print(predicted_labels_dict)\n",
    "    return mean_accuracy/total_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = training_parameters[\"learning_rate\"]\n",
    "n_epochs = training_parameters[\"epochs\"]\n",
    "\n",
    "model = DomainAdaptationModel()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "loss_fn_sentiment_classifier = torch.nn.NLLLoss()\n",
    "loss_fn_domain_classifier = torch.nn.NLLLoss()\n",
    "'''\n",
    "In one training step we will update the model using both the source labeled data and target unlabeled data\n",
    "We will run it till the batches last for any of these datasets\n",
    "\n",
    "In our case target dataset has more data. Hence, we will leverage the entire source dataset for training\n",
    "\n",
    "If we use the same approach in a case where the source dataset has more data then the target dataset then we will\n",
    "under-utilize the labeled source dataset. In such a scenario it is better to reload the target dataset when it finishes\n",
    "This will ensure that we are utilizing the entire source dataset to train our model.\n",
    "'''\n",
    "\n",
    "max_batches = min(len(source_dataloader), len(target_dataloader))\n",
    "\n",
    "for epoch_idx in range(n_epochs):\n",
    "    \n",
    "    source_iterator = iter(source_dataloader)\n",
    "    target_iterator = iter(target_dataloader)\n",
    "\n",
    "    for batch_idx in range(max_batches):\n",
    "        \n",
    "        p = float(batch_idx + epoch_idx * max_batches) / (training_parameters[\"epochs\"] * max_batches)\n",
    "        grl_lambda = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        grl_lambda = torch.tensor(grl_lambda)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        if(batch_idx%training_parameters[\"print_after_steps\"] == 0 ):\n",
    "            print(\"Training Step:\", batch_idx)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Souce dataset training update\n",
    "        input_ids, attention_mask, token_type_ids, labels = next(source_iterator)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\" : token_type_ids,\n",
    "            \"labels\" : labels,\n",
    "            \"grl_lambda\" : grl_lambda,\n",
    "        }\n",
    "\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "    \n",
    "        sentiment_pred, domain_pred = model(**inputs)\n",
    "        loss_s_sentiment = loss_fn_sentiment_classifier(sentiment_pred, inputs[\"labels\"])\n",
    "        y_s_domain = torch.zeros(training_parameters[\"batch_size\"], dtype=torch.long).to(device)\n",
    "        loss_s_domain = loss_fn_domain_classifier(domain_pred, y_s_domain)\n",
    "\n",
    "\n",
    "        # Target dataset training update \n",
    "        input_ids, attention_mask, token_type_ids, labels = next(target_iterator)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\" : token_type_ids,\n",
    "            \"labels\" : labels,\n",
    "            \"grl_lambda\" : grl_lambda,\n",
    "        }\n",
    "\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "    \n",
    "        _, domain_pred = model(**inputs)\n",
    "        \n",
    "        # Note that we are not using the sentiment predictions here for updating the weights\n",
    "        y_t_domain = torch.ones(training_parameters[\"batch_size\"], dtype=torch.long).to(device)\n",
    "        loss_t_domain = loss_fn_domain_classifier(domain_pred, y_t_domain)\n",
    "\n",
    "        # Combining the loss \n",
    "\n",
    "        loss = loss_s_sentiment + loss_s_domain + loss_t_domain\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model after every epoch\n",
    "    \n",
    "    torch.save(model.state_dict(), os.path.join(training_parameters[\"output_folder\"], \"epoch_\" + str(epoch_idx)  +  training_parameters[\"output_file\"] ))\n",
    "    accuracy = evaluate(model, dataset = \"amazon\", percentage = 1).item()\n",
    "    print(\"Accuracy on amazon after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))\n",
    "\n",
    "    accuracy = evaluate(model, dataset = \"imdb\", percentage = 1).item()\n",
    "    print(\"Accuracy on imdb after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model on the entire dev set at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(model, dataset = \"amazon\", percentage = 100).item()         \n",
    "print(\"Accuracy on full amazon is \" + str(accuracy))\n",
    "                                                                                \n",
    "accuracy = evaluate(model, dataset = \"imdb\", percentage = 100).item()            \n",
    "print(\"Accuracy on full imdb is \" + str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
